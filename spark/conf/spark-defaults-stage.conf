# ===== Master =====
spark.master=spark://spark:7077

# ===== Python =====
spark.pyspark.python=/home/sparkuser/.venv/bin/python
spark.pyspark.driver.python=/home/sparkuser/.venv/bin/python
spark.executorEnv.PYSPARK_PYTHON=/home/sparkuser/.venv/bin/python

# ===== JARs =====
# spark.jars=/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.7.2.jar,\
# /opt/spark/jars/spark-hive_2.12-3.5.4.jar,\
# /opt/spark/jars/hadoop-aws-3.3.6.jar,\
# /opt/spark/jars/hadoop-auth-3.3.6.jar,\
# /opt/spark/jars/hadoop-common-3.3.6.jar,\
# /opt/spark/jars/hadoop-client-api-3.3.6.jar,\
# /opt/spark/jars/hadoop-client-runtime-3.3.6.jar,\
# /opt/spark/jars/aws-java-sdk-bundle-1.12.734.jar

spark.driver.extraClassPath=/opt/spark-jobs/db_drivers/*
spark.executor.extraClassPath=/opt/spark-jobs/db_drivers/*

# ===== S3A / MinIO =====
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# ===== Hive Catalog =====
spark.sql.catalogImplementation=hive

# ===== Escrita / Desempenho =====
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.parquet.block.size=268435456
spark.sql.files.maxRecordsPerFile=1000000
spark.sql.catalog.tjce_raw_jud.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_raw_adm.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_trt_jud.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_trt_adm.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_rfn_jud.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_rfn_adm.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_sbx_jud.write.parquet.compression-codec = zstd
spark.sql.catalog.tjce_sbx_adm.write.parquet.compression-codec = zstd


# ===== Legacy / Committers =====
spark.sql.parquet.int96RebaseModeInWrite=LEGACY
spark.sql.parquet.datetimeRebaseModeInWrite=LEGACY
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.fs.s3a.committer.name=partitioned
spark.sql.optimizer.charTypePropagation.enabled=false
spark.sql.parquet.outputTimestampType=TIMESTAMP_MICROS

# ===== Alocação dinâmica (requer shuffle externo nos workers) =====
spark.shuffle.service.enabled=true
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.initialExecutors=1
spark.dynamicAllocation.maxExecutors=3

# ===== Tamanho dos executores =====
spark.executor.cores=2
spark.executor.memory=6g
spark.executor.memoryOverhead=1g

# ===== Limites por aplicação =====
spark.cores.max=6

# ===== Paralelismo =====
spark.default.parallelism=64
spark.sql.shuffle.partitions=200

# ===== Driver =====
spark.driver.memory=4g

# ===== Heartbeat =====
spark.network.timeout=300s
spark.executor.heartbeatInterval=60s